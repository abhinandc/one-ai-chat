---
phase: 01-document-processing-fix
plan: 01
type: execute
depends_on: []
files_modified: [supabase/functions/llm-proxy/index.ts]
---

<objective>
Fix document/image processing to use OpenAI/Claude vision by default instead of OneAI OCR.

Purpose: OneAI OCR (op3 model) is failing silently. Users need reliable document processing using proven vision APIs from OpenAI and Claude.
Output: Smart attachment routing that defaults to OpenAI/Claude vision, with OneAI OCR only when explicitly selected.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-plan.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@supabase/functions/llm-proxy/index.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Update vision model detection for Claude 4.x models</name>
  <files>supabase/functions/llm-proxy/index.ts</files>
  <action>
Update the modelSupportsVision() function (lines 25-55) to properly detect ALL Claude 4.x vision models:
- Add 'claude-4' to catch all Claude 4 variants
- Add explicit checks for 'claude-opus-4', 'claude-sonnet-4', 'claude-sonnet-4-5'
- Ensure 'claude-3.5-sonnet', 'claude-3-opus', 'claude-3-sonnet' are covered
- Add GPT-4 Turbo vision variants: 'gpt-4-turbo', 'gpt-4-1106-vision-preview'
- Log which model was checked and the result for debugging

AVOID: Don't remove existing model checks - add to them. Keep backward compatibility.
  </action>
  <verify>Deploy function and check logs show vision detection for Claude 4.x models</verify>
  <done>modelSupportsVision() returns true for claude-opus-4, claude-sonnet-4, claude-sonnet-4-5, gpt-4o, gpt-4-turbo</done>
</task>

<task type="auto">
  <name>Task 2: Fix Anthropic image format conversion</name>
  <files>supabase/functions/llm-proxy/index.ts</files>
  <action>
Fix the convertMessagesForAnthropic() function (lines 320-370) to properly handle image content:

1. Ensure proper base64 data URI parsing with regex: /^data:([^;]+);base64,(.+)$/
2. Handle edge cases:
   - Missing media_type defaults to 'image/png'
   - Handle both 'jpeg' and 'jpg' MIME types
   - Preserve text content parts unchanged
3. Add robust logging to trace conversion issues
4. Handle mixed content (text + images in same message)

Current output format for Anthropic:
```typescript
{
  type: 'image',
  source: {
    type: 'base64',
    media_type: 'image/png', // or jpeg, webp, gif
    data: 'base64stringwithoutprefix'
  }
}
```

AVOID: Don't break OpenAI format - only transform for Anthropic provider.
  </action>
  <verify>Test with a Claude model and image attachment - check logs show proper format conversion</verify>
  <done>Images sent to Claude models use correct Anthropic format with base64 source</done>
</task>

<task type="auto">
  <name>Task 3: Change default document processing to use vision models</name>
  <files>supabase/functions/llm-proxy/index.ts</files>
  <action>
Modify the document processing decision logic (lines 587-664) to:

1. **NEW DEFAULT BEHAVIOR**: If model supports vision AND has image attachments → use vision API directly (no OCR)
2. **FALLBACK TO OCR ONLY IF**:
   - Model explicitly contains 'op3', 'olmocr', 'paddleocr', or 'oneai-ocr' in name
   - OR user explicitly selects a OneAI OCR model
   - OR model doesn't support vision AND has documents
3. **PDF HANDLING**:
   - For PDFs with vision models: Convert first page to image and use vision
   - For PDFs with non-vision models: Use OCR extraction
4. Add clear logging: "[llm-proxy] Routing: Using [vision|ocr] for [model] with [attachment-types]"

Current problematic flow:
- Documents detected → Always use OCR → OneAI OCR fails silently

New flow:
- Images + Vision model → Vision API (OpenAI/Claude)
- Images + Non-vision model → OCR fallback
- PDFs + Vision model → Try vision first, OCR fallback
- Explicit OCR model selected → Use OCR

AVOID: Breaking existing functionality for users who explicitly want OCR.
  </action>
  <verify>Send image to Claude/GPT model → verify it goes through vision API, not OCR. Check logs.</verify>
  <done>Image attachments with vision-capable models bypass OCR and use native vision APIs</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `supabase functions deploy llm-proxy` succeeds
- [ ] Image sent with Claude model shows "[llm-proxy] Using vision API" in logs
- [ ] Image sent with GPT-4o shows "[llm-proxy] Using vision API" in logs
- [ ] OCR model explicitly selected still uses OCR processing
- [ ] No TypeScript compilation errors
</verification>

<success_criteria>
- Vision model detection works for all Claude 4.x and GPT-4 vision variants
- Anthropic image format conversion produces valid base64 image content
- Default routing uses vision APIs for image attachments
- OneAI OCR only used when explicitly selected or as fallback
- Clear logging shows routing decisions
</success_criteria>

<output>
After completion, create `.planning/phases/01-document-processing-fix/01-01-SUMMARY.md`
</output>
